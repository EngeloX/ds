{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b023f87-9f01-4f09-bde4-f11d6ae5f088",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c3876-1e12-42b3-8a08-a0c17c721447",
   "metadata": {},
   "source": [
    "1) Инициализация $predictions$ константной статистикой (AVG)\n",
    "2) Расчёт $residuals$ ( $-\\nabla\\mathcal L(y, predictions)$ )\n",
    "3) Обучение **Weak Learner**(*DecisionTree*) на $X$ и $residuals$\\\n",
    "3.5) Обученный ученик сохраняется в ансамбль\n",
    "5) Прогноз **Weak Learner** на $X$ умноженный на *learning_rate* прибавляется к $predictions$\n",
    "6) Шаги 2-4 повторяются $n$ итераций (*n_estimators*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4952e02-777b-406f-815e-a002c54e010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7bc19065-44a2-4877-9953-250e442a3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoosting:\n",
    "    def __init__(self, n_estimators=100, max_depth=3, min_samples_leaf=1, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.trees = []\n",
    "        self.residuals = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.first_leaf = None\n",
    "        self.train_score = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # 1. Инициализация Predictions средним арифметическим\n",
    "        self.first_leaf = y.mean()\n",
    "        predictions = np.ones(len(y)) * self.first_leaf\n",
    "\n",
    "        # 5. Цикл обучения\n",
    "        for i in range(self.n_estimators):\n",
    "            # 2. Антиградиент MSE\n",
    "            residuals = y - predictions\n",
    "            self.residuals.append(residuals)\n",
    "\n",
    "            # 3. Обучение Tree на X и Остатках(Антиградиент)\n",
    "            tree = DecisionTreeRegressor(max_depth = self.max_depth, min_samples_leaf = self.min_samples_leaf)\n",
    "            tree.fit(X, residuals)\n",
    "            # 3.5 Сохранение дерева в ансамбль\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            # 4. Увеличение predictions на прогноз\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "    def predict(self, X):           \n",
    "        predictions = np.ones(X.shape[0]) * self.first_leaf\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            predictions += self.learning_rate * self.trees[i].predict(X)\n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "52c7a7fe-993d-49c7-80cd-080a2b6e2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_regression(n_samples=2000, n_features=5, n_informative=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "97d5b495-855c-41ba-9c64-537d9a4cc7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GB = GradientBoosting(n_estimators=100, min_samples_leaf=10, max_depth=3, learning_rate=0.1)\n",
    "GB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d2e6c5bf-025d-44d0-9229-c6f79abeeb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = GB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c61ea6dd-e161-4655-866f-2f955f8116d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9931373404624239"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "11.100506151801872"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(r2_score(y_test, y_pred), \n",
    "        root_mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "66ff097c-0a30-4e98-83dd-5b35c44e0cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "skGB = GradientBoostingRegressor(n_estimators=100, min_samples_leaf=10, max_depth=3, learning_rate=0.1)\n",
    "skGB.fit(X_train, y_train)\n",
    "y_pred = skGB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e7d9278a-8509-4602-abca-656b12eecec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9931373404624239"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "11.100506151801879"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(r2_score(y_test, y_pred), \n",
    "        root_mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc7787b-4ce0-42d4-bbee-ae11223e807d",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0e779c-bdc5-4989-8a8e-0c67c68743b9",
   "metadata": {},
   "source": [
    "Полная модель бустингового ансамбля:\n",
    "$$\n",
    "F(x) = f_0(x)+\\sum_{i=1}^T\\eta f_t(x)\n",
    "$$\n",
    "$f_0(x)$ - Начальное приближение, инициализация начальных константой ( Среднее арифметическое )\\\n",
    "$T$ - Число деревьев в ансамбле ( `n_estimators` )\\\n",
    "$f_t(x)$ - Предсказание дерева на $t$-итерации\\\n",
    "$\\eta$ - Шаг обучения ( `learning_rate` )\n",
    "\n",
    "---\n",
    "\n",
    "Построить такую модель $F(x)$, которая минимизируеи эмпирический риск, то есть ошибка по результатам ансамбля будет минимально возможная в данной ситуации.\n",
    "$$\n",
    "F^*(x) = \\arg\\min_{F} \\sum_{i=1}^n\\mathcal L(y_i, F(x_i))    \\hspace{1cm}  \\mathcal L(y_i, \\hat{y_i})\\rightarrow min\n",
    "$$\n",
    "$F^*(x)$ - Оптимальная модель\\\n",
    "$\\arg\\min_{F}$ - Выбор из всех функций $F$ такую, которая выдает минимальное суммарное значение **ошибки** по всем объектам\\\n",
    "$F(x_i)$=$\\hat{y_i}$ - Предсказание модели \\\n",
    "$\\mathcal L(y_i, F(x_i)$ - Функция потерь измеряющая ошибку на $i$-ом объекте\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "**Полная функция потерь**, минимизируемая на каждом шаге $t$ состоит из двух слогаемых - самой функции потерь и регуляризационной накопительной\n",
    "$$\n",
    "\\mathcal L(y_i, \\hat{y_i})= \\sum_{i=1}^nL(y_i, \\hat{y_i}) + \\sum_{k=1}^t\\Omega f_k(x)\n",
    "$$\n",
    "$$\n",
    "\\Omega (f_k(x)) = \\gamma NumLeaves + \\frac{1}{2} \\lambda \\sum^T_{j=1}{w_j^2}\n",
    "$$\n",
    "\n",
    "$\\mathcal L(y_i, \\hat{y_i})$ -  Полная функция функция потерь\\\n",
    "$\\sum_{i=1}^nL(y_i, \\hat{y_i})$ - Накопительная ошибка (Суммарная ошибка по всем объектам)\\\n",
    "$\\sum_{k=1}^t\\Omega f_k$ - Накопительный штраф (Суммарный штраф по всем деревьям)\n",
    "\n",
    "$n$ - количество объектов( строк данных )\\\n",
    "$t$ - количество деревьев( `n_estimators` )\\\n",
    "$L(y_i, \\hat{y_i})$ - Выбранная функция потерь\\\n",
    "$f_k$ - $k$-ое дерево в ансамбле ( $k = 1,2,3,...,t$ )\\\n",
    "$\\Omega f_k(x)$ - Регуляризационный штраф за структуру $k$-го дерева\\\n",
    "$NumLeaves$ - Количество листьев в дереве $f_k$\\\n",
    "$\\lambda$ - Коэффициент L2-регуляризации весов ( `reg_lambda` ) \\\n",
    "$\\gamma$ - Коэффициент регуляризации для числа листьев. Штраф за каждый новый лист. (`gamma`)\\\n",
    "$w_j$ - Вес $j$-го листа.\n",
    "\n",
    "------\n",
    "На каждом шаге $t$ для каждого объекта $i$ функция потерь $\\mathcal L(y_i, F(x_i)$ разлагается в Ряд Тейлора до второго порядка в окрестностях текущего предсказания.\n",
    "$$\n",
    "\\mathcal L(y_i, F(x_i) \\approx \\mathcal L(y_i, F_{t-1}(x_i))+g_if_t(x_i) + \\frac{1}{2}h_if_t(x_i)^2 + \\Omega (f_t)\n",
    "$$\n",
    "$$\n",
    "\\mathcal L^{(t)} \\approx \\sum_{i=1}^n \\Big[ g_if_t(x_i)+\\frac{1}{2}h_if_t(x_i)^2\\Big] + \\Omega (f_t)\n",
    "$$\n",
    "$f_t(x_i)$ - Новое предсказание дерева $t$-итерации для объекта $x_i$\\\n",
    "$g_j$ - Градиент значений $j$-листа (Производная по предсказанию)\\\n",
    "$h_j$ - Гессиан значений $j$-листа (Производная второго порядка по предсказанию)\n",
    "$$\n",
    "g_i = \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\hspace{2cm} h_i\\frac{\\partial^2 L(y_i, F(x_i))}{\\partial F(x_i)^2} \n",
    "$$\n",
    "Минимизация функции потерь осуществляется с использованием этих разложений. Ищется такой шаг $f_t(x)$  который минимизирует ошибку. Для этого учитывается как градиент, так и гессиан, которые нам показывают, как наилучшим образом обновить предсказание.\n",
    "\n",
    "Оптимизации методом градиентного спуска недостаточно для минимизации ошибки. Вместо этого, используется разложение в ряд Тейлора, чтобы аппроксимировать, как будет изменяться ошибка, если мы добавим предсказания от нового дерева. Таким образом, обучение дерева сводится к минимизации разложения по текущим градиентам и гессианам. \n",
    "\n",
    "\n",
    "Алгоритм построения дерево вычисляет оценку качества(Information Gain) для каждого разбиения:\n",
    "$$\n",
    "IG = \\frac{1}{2} \\Big[\\frac{G^2_{L}}{H_{L}+\\lambda}+\\frac{G^2_{R}}{H_{R}+\\lambda} - \\frac{(G_L+G_R)^2}{H_L + H_R + \\lambda}\\Big] - \\gamma\n",
    "$$\n",
    "\n",
    "$$\n",
    "G_{L/R} = \\sum_{i \\in l/r} g_i \\hspace{2cm} H_{L/R} = \\sum_{i \\in l/r} h_i \\hspace{2cm}\n",
    "$$\n",
    "$G_L$, $G_R$ - Суммы **градиентов** в левом и правом подмножестве после разбиения\\\n",
    "$H_L$, $G_R$ - Суммы **гессианов** в левом и правом подмножестве после разбиения\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-------\n",
    "$w_j$ -Прогноз выдоваемый $j$-ым листом. Решение задачи оптимизации через производные.\\\n",
    "Агрегированный прогноз по всем объектам в этом листе, но скорректированный так, чтобы минимизировать функцию потерь через производные первого и второго порядка. \n",
    "$$\n",
    "w_j = - \\frac{G_j}{H_j+\\lambda}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "w_j = - \\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}\n",
    "$$\n",
    "\n",
    "$I_j$ - Множество объектов, попавших в лист $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad3c90-fe3c-44b2-8668-252c9e6c4642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
